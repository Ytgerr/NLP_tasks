{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa910b5d",
   "metadata": {},
   "source": [
    "#### Download libraries if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01a69ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\k4ty2\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in c:\\users\\k4ty2\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (20250327)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\k4ty2\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pdfminer.six) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\k4ty2\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pdfminer.six) (45.0.3)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\k4ty2\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\k4ty2\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f890f",
   "metadata": {},
   "source": [
    "#### Import libraries (NumPy + Re + one library to read the .pdf file,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87816893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44858ea",
   "metadata": {},
   "source": [
    "#### Read the text from pdf using pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e33fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_pdf(pdf_path):\n",
    "    text_per_page = {}\n",
    "    for pagenum, page in enumerate(extract_pages(pdf_path)):\n",
    "        page_content = []\n",
    "        page_elements = [element for element in page._objs]\n",
    "        for element in page_elements:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                line_text = element.get_text()\n",
    "                page_content.append(line_text)  \n",
    "        text_per_page[pagenum] = ''.join(page_content)\n",
    "    return ''.join(text_per_page.values())\n",
    "\n",
    "pdf_path1 = '../vector_word.pdf'\n",
    "pdf_path2 = '../transformers.pdf'\n",
    "text1 = read_pdf(pdf_path1)\n",
    "text2 = read_pdf(pdf_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd453b82",
   "metadata": {},
   "source": [
    "#### Text preprocessing\n",
    "- make stop word list\n",
    "- remove special characters and tags from the documents\n",
    "- split text on word using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d061974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['distributed',\n",
       " 'representations',\n",
       " 'words',\n",
       " 'phrases',\n",
       " 'compositionality',\n",
       " 'tomas',\n",
       " 'mikolov',\n",
       " 'google',\n",
       " 'mountain',\n",
       " 'view',\n",
       " 'mikolov',\n",
       " 'google',\n",
       " 'ilya',\n",
       " 'sutskever',\n",
       " 'google',\n",
       " 'mountain',\n",
       " 'view',\n",
       " 'ilyasu',\n",
       " 'google',\n",
       " 'chen',\n",
       " 'google',\n",
       " 'mountain',\n",
       " 'view',\n",
       " 'google',\n",
       " 'greg',\n",
       " 'corrado',\n",
       " 'google',\n",
       " 'mountain',\n",
       " 'view',\n",
       " 'gcorrado',\n",
       " 'google',\n",
       " 'jeffrey',\n",
       " 'dean',\n",
       " 'google',\n",
       " 'mountain',\n",
       " 'view',\n",
       " 'jeff',\n",
       " 'google',\n",
       " 'abstract',\n",
       " 'recently',\n",
       " 'introduced',\n",
       " 'continuous',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'model',\n",
       " 'efcient',\n",
       " 'method',\n",
       " 'learning',\n",
       " 'high',\n",
       " 'quality',\n",
       " 'distributed',\n",
       " 'vector',\n",
       " 'representations',\n",
       " 'capture',\n",
       " 'large',\n",
       " 'num',\n",
       " 'ber',\n",
       " 'precise',\n",
       " 'syntactic',\n",
       " 'semantic',\n",
       " 'word',\n",
       " 'relationships',\n",
       " 'paper',\n",
       " 'present',\n",
       " 'several',\n",
       " 'extensions',\n",
       " 'improve',\n",
       " 'both',\n",
       " 'quality',\n",
       " 'vectors',\n",
       " 'training',\n",
       " 'speed',\n",
       " 'subsampling',\n",
       " 'frequent',\n",
       " 'words',\n",
       " 'obtain',\n",
       " 'signicant',\n",
       " 'speedup',\n",
       " 'learn',\n",
       " 'more',\n",
       " 'regular',\n",
       " 'word',\n",
       " 'representations',\n",
       " 'describe',\n",
       " 'simple',\n",
       " 'alterna',\n",
       " 'tive',\n",
       " 'hierarchical',\n",
       " 'softmax',\n",
       " 'called',\n",
       " 'negative',\n",
       " 'sampling',\n",
       " 'inherent',\n",
       " 'limitation',\n",
       " 'word',\n",
       " 'representations',\n",
       " 'indifference',\n",
       " 'word',\n",
       " 'order',\n",
       " 'inability',\n",
       " 'represent',\n",
       " 'idiomatic',\n",
       " 'phrases',\n",
       " 'example',\n",
       " 'meanings',\n",
       " 'canada',\n",
       " 'air',\n",
       " 'cannot',\n",
       " 'easily',\n",
       " 'combined',\n",
       " 'obtain',\n",
       " 'air',\n",
       " 'canada',\n",
       " 'motivated',\n",
       " 'example',\n",
       " 'present',\n",
       " 'simple',\n",
       " 'method',\n",
       " 'nding',\n",
       " 'phrases',\n",
       " 'text',\n",
       " 'show',\n",
       " 'learning',\n",
       " 'good',\n",
       " 'vector',\n",
       " 'representations',\n",
       " 'millions',\n",
       " 'phrases',\n",
       " 'possible',\n",
       " 'introduction',\n",
       " 'distributed',\n",
       " 'representations',\n",
       " 'words',\n",
       " 'vector',\n",
       " 'space',\n",
       " 'help',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'achieve',\n",
       " 'better',\n",
       " 'performance',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'tasks',\n",
       " 'grouping',\n",
       " 'similar',\n",
       " 'words',\n",
       " 'one',\n",
       " 'earliest',\n",
       " 'use',\n",
       " 'word',\n",
       " 'representations',\n",
       " 'dates',\n",
       " 'back',\n",
       " 'rumelhart',\n",
       " 'hinton',\n",
       " 'williams',\n",
       " 'idea',\n",
       " 'has',\n",
       " 'since',\n",
       " 'been',\n",
       " 'applied',\n",
       " 'statistical',\n",
       " 'language',\n",
       " 'modeling',\n",
       " 'considerable',\n",
       " 'success',\n",
       " 'follow',\n",
       " 'up',\n",
       " 'work',\n",
       " 'includes',\n",
       " 'applications',\n",
       " 'automatic',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'nlp',\n",
       " 'tasks',\n",
       " 'recently',\n",
       " 'mikolov',\n",
       " 'introduced',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'model',\n",
       " 'efcient',\n",
       " 'method',\n",
       " 'learning',\n",
       " 'high',\n",
       " 'quality',\n",
       " 'vector',\n",
       " 'representations',\n",
       " 'words',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'unstructured',\n",
       " 'text',\n",
       " 'data',\n",
       " 'unlike',\n",
       " 'most',\n",
       " 'previously',\n",
       " 'used',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'architectures',\n",
       " 'learning',\n",
       " 'word',\n",
       " 'vectors',\n",
       " 'training',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'model',\n",
       " 'see',\n",
       " 'does',\n",
       " 'not',\n",
       " 'involve',\n",
       " 'dense',\n",
       " 'matrix',\n",
       " 'multiplications',\n",
       " 'makes',\n",
       " 'training',\n",
       " 'extremely',\n",
       " 'efcient',\n",
       " 'optimized',\n",
       " 'single',\n",
       " 'machine',\n",
       " 'implementation',\n",
       " 'can',\n",
       " 'train',\n",
       " 'more',\n",
       " 'billion',\n",
       " 'words',\n",
       " 'one',\n",
       " 'day',\n",
       " 'word',\n",
       " 'representations',\n",
       " 'computed',\n",
       " 'using',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'very',\n",
       " 'interesting',\n",
       " 'because',\n",
       " 'learned',\n",
       " 'vectors',\n",
       " 'explicitly',\n",
       " 'encode',\n",
       " 'many',\n",
       " 'linguistic',\n",
       " 'regularities',\n",
       " 'patterns',\n",
       " 'somewhat',\n",
       " 'surprisingly',\n",
       " 'many',\n",
       " 'these',\n",
       " 'patterns',\n",
       " 'can',\n",
       " 'represented',\n",
       " 'linear',\n",
       " 'translations',\n",
       " 'example',\n",
       " 'result',\n",
       " 'vector',\n",
       " 'calcula',\n",
       " 'vecmadrid',\n",
       " 'vecspain',\n",
       " 'vecfrance',\n",
       " 'closer',\n",
       " 'vecparis',\n",
       " 'any',\n",
       " 'word',\n",
       " 'vector',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'cid',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'model',\n",
       " 'architecture',\n",
       " 'training',\n",
       " 'objective',\n",
       " 'learn',\n",
       " 'word',\n",
       " 'vector',\n",
       " 'representations',\n",
       " 'good',\n",
       " 'predicting',\n",
       " 'nearby',\n",
       " 'words',\n",
       " 'paper',\n",
       " 'present',\n",
       " 'several',\n",
       " 'extensions',\n",
       " 'original',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'model',\n",
       " 'show',\n",
       " 'sub',\n",
       " 'sampling',\n",
       " 'frequent',\n",
       " 'words',\n",
       " 'during',\n",
       " 'training',\n",
       " 'results',\n",
       " 'signicant',\n",
       " 'speedup',\n",
       " 'around',\n",
       " 'improves',\n",
       " 'accuracy',\n",
       " 'representations',\n",
       " 'less',\n",
       " 'frequent',\n",
       " 'words',\n",
       " 'addition',\n",
       " 'present',\n",
       " 'simpli',\n",
       " 'ed',\n",
       " 'variant',\n",
       " 'noise',\n",
       " 'contrastive',\n",
       " 'estimation',\n",
       " 'nce',\n",
       " 'training',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'model',\n",
       " 'results',\n",
       " 'faster',\n",
       " 'training',\n",
       " 'better',\n",
       " 'vector',\n",
       " 'representations',\n",
       " 'frequent',\n",
       " 'words',\n",
       " 'compared',\n",
       " 'more',\n",
       " 'complex',\n",
       " 'hierarchical',\n",
       " 'softmax',\n",
       " 'was',\n",
       " 'used',\n",
       " 'prior',\n",
       " 'work',\n",
       " 'word',\n",
       " 'representations',\n",
       " 'limited',\n",
       " 'inability',\n",
       " 'represent',\n",
       " 'idiomatic',\n",
       " 'phrases',\n",
       " 'not',\n",
       " 'positions',\n",
       " 'individual',\n",
       " 'words',\n",
       " 'example',\n",
       " 'boston',\n",
       " 'globe',\n",
       " 'newspaper',\n",
       " 'so',\n",
       " 'not',\n",
       " 'natural',\n",
       " 'combination',\n",
       " 'meanings',\n",
       " 'boston',\n",
       " 'globe',\n",
       " 'therefore',\n",
       " 'using',\n",
       " 'vectors',\n",
       " 'repre',\n",
       " 'sent',\n",
       " 'whole',\n",
       " 'phrases',\n",
       " 'makes',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'model',\n",
       " 'considerably',\n",
       " 'more',\n",
       " 'expressive',\n",
       " 'techniques',\n",
       " 'aim',\n",
       " 'represent',\n",
       " 'meaning',\n",
       " 'sentences',\n",
       " 'composing',\n",
       " 'word',\n",
       " 'vectors',\n",
       " 'recursive',\n",
       " 'autoencoders',\n",
       " 'would',\n",
       " 'benet',\n",
       " 'using',\n",
       " 'phrase',\n",
       " 'vectors',\n",
       " 'instead',\n",
       " 'word',\n",
       " 'vectors',\n",
       " 'extension',\n",
       " 'word',\n",
       " 'based',\n",
       " 'phrase',\n",
       " 'based',\n",
       " 'models',\n",
       " 'relatively',\n",
       " 'simple',\n",
       " 'first',\n",
       " 'identify',\n",
       " 'large',\n",
       " 'number',\n",
       " 'phrases',\n",
       " 'using',\n",
       " 'data',\n",
       " 'driven',\n",
       " 'approach',\n",
       " 'then',\n",
       " 'treat',\n",
       " 'phrases',\n",
       " 'individual',\n",
       " 'tokens',\n",
       " 'during',\n",
       " 'training',\n",
       " 'evaluate',\n",
       " 'quality',\n",
       " 'phrase',\n",
       " 'vectors',\n",
       " 'developed',\n",
       " 'test',\n",
       " 'set',\n",
       " 'analogi',\n",
       " 'cal',\n",
       " 'reasoning',\n",
       " 'tasks',\n",
       " 'contains',\n",
       " 'both',\n",
       " 'words',\n",
       " 'phrases',\n",
       " 'typical',\n",
       " 'analogy',\n",
       " 'pair',\n",
       " 'test',\n",
       " 'set',\n",
       " 'montreal',\n",
       " 'montreal',\n",
       " 'canadiens',\n",
       " 'toronto',\n",
       " 'toronto',\n",
       " 'maple',\n",
       " 'leafs',\n",
       " 'considered',\n",
       " 'have',\n",
       " 'been',\n",
       " 'answered',\n",
       " 'correctly',\n",
       " 'if',\n",
       " 'nearest',\n",
       " 'representation',\n",
       " 'vecmontreal',\n",
       " 'canadiens',\n",
       " 'vecmontreal',\n",
       " 'vectoronto',\n",
       " 'vectoronto',\n",
       " 'maple',\n",
       " 'leafs',\n",
       " 'finally',\n",
       " 'describe',\n",
       " 'another',\n",
       " 'interesting',\n",
       " 'property',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'model',\n",
       " 'found',\n",
       " 'simple',\n",
       " 'vector',\n",
       " 'addition',\n",
       " 'can',\n",
       " 'often',\n",
       " 'produce',\n",
       " 'meaningful',\n",
       " 'results',\n",
       " 'example',\n",
       " 'vecrussia',\n",
       " 'vecriver',\n",
       " 'close',\n",
       " 'vecvolga',\n",
       " 'river',\n",
       " 'vecgermany',\n",
       " 'veccapital',\n",
       " 'close',\n",
       " 'vecberlin',\n",
       " 'compositionality',\n",
       " 'suggests',\n",
       " 'non',\n",
       " 'obvious',\n",
       " 'degree',\n",
       " 'language',\n",
       " 'understanding',\n",
       " 'can',\n",
       " 'obtained',\n",
       " 'using',\n",
       " 'basic',\n",
       " 'mathematical',\n",
       " 'operations',\n",
       " 'word',\n",
       " 'vector',\n",
       " 'representations',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'model',\n",
       " 'training',\n",
       " 'objective',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'model',\n",
       " 'nd',\n",
       " 'word',\n",
       " 'representations',\n",
       " 'useful',\n",
       " 'predicting',\n",
       " 'surrounding',\n",
       " 'words',\n",
       " 'sentence',\n",
       " 'document',\n",
       " 'more',\n",
       " 'formally',\n",
       " 'given',\n",
       " 'sequence',\n",
       " 'training',\n",
       " 'words',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'wt',\n",
       " 'objective',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'model',\n",
       " 'maximize',\n",
       " 'average',\n",
       " 'log',\n",
       " 'probability',\n",
       " 'cjcj',\n",
       " 'log',\n",
       " 'pwtj',\n",
       " 'wt',\n",
       " 'where',\n",
       " 'size',\n",
       " 'training',\n",
       " 'context',\n",
       " 'which',\n",
       " 'can',\n",
       " 'function',\n",
       " 'center',\n",
       " 'word',\n",
       " 'wt',\n",
       " 'larger',\n",
       " 'results',\n",
       " 'more',\n",
       " 'training',\n",
       " 'examples',\n",
       " 'can',\n",
       " 'lead',\n",
       " 'higher',\n",
       " 'accuracy',\n",
       " 'expense',\n",
       " 'training',\n",
       " 'time',\n",
       " 'basic',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'formulation',\n",
       " 'denes',\n",
       " 'pwtj',\n",
       " 'wt',\n",
       " 'using',\n",
       " 'softmax',\n",
       " 'function',\n",
       " 'pwowi',\n",
       " 'exp',\n",
       " 'wo',\n",
       " 'vwi',\n",
       " 'cid',\n",
       " 'w',\n",
       " 'w',\n",
       " 'exp',\n",
       " 'cid',\n",
       " 'vwi',\n",
       " 'w',\n",
       " 'cid',\n",
       " 'where',\n",
       " 'vw',\n",
       " 'w',\n",
       " 'input',\n",
       " 'output',\n",
       " 'vector',\n",
       " 'representations',\n",
       " 'w',\n",
       " 'w',\n",
       " 'num',\n",
       " 'ber',\n",
       " 'words',\n",
       " 'vocabulary',\n",
       " 'formulation',\n",
       " 'impractical',\n",
       " 'because',\n",
       " 'cost',\n",
       " 'computing',\n",
       " 'log',\n",
       " 'pwowi',\n",
       " 'proportional',\n",
       " 'w',\n",
       " 'which',\n",
       " 'often',\n",
       " 'large',\n",
       " 'terms',\n",
       " 'p',\n",
       " 'cid',\n",
       " 'hierarchical',\n",
       " 'softmax',\n",
       " 'computationally',\n",
       " 'efcient',\n",
       " 'approximation',\n",
       " 'full',\n",
       " 'softmax',\n",
       " 'hierarchical',\n",
       " 'softmax',\n",
       " 'context',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'language',\n",
       " 'models',\n",
       " 'was',\n",
       " 'rst',\n",
       " 'introduced',\n",
       " 'morin',\n",
       " 'bengio',\n",
       " 'main',\n",
       " 'advantage',\n",
       " 'instead',\n",
       " 'evaluating',\n",
       " 'w',\n",
       " 'output',\n",
       " 'nodes',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'obtain',\n",
       " 'probability',\n",
       " 'distribution',\n",
       " 'needed',\n",
       " 'evaluate',\n",
       " 'only',\n",
       " 'logw',\n",
       " 'nodes',\n",
       " 'hierarchical',\n",
       " 'softmax',\n",
       " 'binary',\n",
       " 'tree',\n",
       " 'representation',\n",
       " 'output',\n",
       " 'layer',\n",
       " 'w',\n",
       " 'words',\n",
       " 'leaves',\n",
       " 'each',\n",
       " 'node',\n",
       " 'explicitly',\n",
       " 'represents',\n",
       " 'relative',\n",
       " 'probabilities',\n",
       " 'child',\n",
       " 'nodes',\n",
       " 'these',\n",
       " 'dene',\n",
       " 'random',\n",
       " 'walk',\n",
       " 'assigns',\n",
       " 'probabilities',\n",
       " 'words',\n",
       " 'more',\n",
       " 'precisely',\n",
       " 'each',\n",
       " 'word',\n",
       " 'w',\n",
       " 'can',\n",
       " 'reached',\n",
       " 'appropriate',\n",
       " 'path',\n",
       " 'root',\n",
       " 'tree',\n",
       " 'let',\n",
       " 'nw',\n",
       " 'node',\n",
       " 'path',\n",
       " 'root',\n",
       " 'w',\n",
       " 'let',\n",
       " 'lw',\n",
       " 'length',\n",
       " 'path',\n",
       " 'so',\n",
       " 'nw',\n",
       " 'root',\n",
       " 'nw',\n",
       " 'lw',\n",
       " 'w',\n",
       " 'addition',\n",
       " 'any',\n",
       " 'inner',\n",
       " 'node',\n",
       " 'let',\n",
       " 'chn',\n",
       " 'arbitrary',\n",
       " 'xed',\n",
       " 'child',\n",
       " 'let',\n",
       " 'if',\n",
       " 'true',\n",
       " 'otherwise',\n",
       " 'then',\n",
       " 'hierarchical',\n",
       " 'softmax',\n",
       " 'denes',\n",
       " 'pwowi',\n",
       " 'follows',\n",
       " 'lw',\n",
       " 'pwwi',\n",
       " 'nw',\n",
       " 'chnw',\n",
       " 'cid',\n",
       " 'nwj',\n",
       " 'vwi',\n",
       " 'cid',\n",
       " 'w',\n",
       " 'w',\n",
       " 'pwwi',\n",
       " 'implies',\n",
       " 'where',\n",
       " 'expx',\n",
       " 'can',\n",
       " 'veried',\n",
       " 'cost',\n",
       " 'computing',\n",
       " 'log',\n",
       " 'pwowi',\n",
       " 'log',\n",
       " 'pwowi',\n",
       " 'proportional',\n",
       " 'lwo',\n",
       " 'which',\n",
       " 'average',\n",
       " 'no',\n",
       " 'greater',\n",
       " 'log',\n",
       " 'w',\n",
       " 'unlike',\n",
       " 'standard',\n",
       " 'softmax',\n",
       " 'formulation',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'which',\n",
       " 'assigns',\n",
       " 'two',\n",
       " 'representations',\n",
       " 'vw',\n",
       " 'w',\n",
       " 'each',\n",
       " 'word',\n",
       " 'w',\n",
       " 'hierarchical',\n",
       " 'softmax',\n",
       " 'formulation',\n",
       " 'has',\n",
       " 'one',\n",
       " 'representation',\n",
       " 'vw',\n",
       " 'each',\n",
       " 'word',\n",
       " 'w',\n",
       " 'one',\n",
       " 'representation',\n",
       " 'every',\n",
       " 'inner',\n",
       " 'node',\n",
       " 'binary',\n",
       " 'tree',\n",
       " 'p',\n",
       " 'structure',\n",
       " 'tree',\n",
       " 'used',\n",
       " 'hierarchical',\n",
       " 'softmax',\n",
       " 'has',\n",
       " 'considerable',\n",
       " 'effect',\n",
       " 'perfor',\n",
       " 'mance',\n",
       " 'mnih',\n",
       " 'hinton',\n",
       " 'explored',\n",
       " 'number',\n",
       " 'methods',\n",
       " 'constructing',\n",
       " 'tree',\n",
       " 'structure',\n",
       " 'effect',\n",
       " 'both',\n",
       " 'training',\n",
       " 'time',\n",
       " 'resulting',\n",
       " 'model',\n",
       " 'accuracy',\n",
       " 'work',\n",
       " 'use',\n",
       " 'binary',\n",
       " 'huffman',\n",
       " 'tree',\n",
       " 'assigns',\n",
       " 'short',\n",
       " 'codes',\n",
       " 'frequent',\n",
       " 'words',\n",
       " 'which',\n",
       " 'results',\n",
       " 'fast',\n",
       " 'training',\n",
       " 'has',\n",
       " 'been',\n",
       " 'observed',\n",
       " 'before',\n",
       " 'grouping',\n",
       " 'words',\n",
       " 'together',\n",
       " 'frequency',\n",
       " 'works',\n",
       " 'well',\n",
       " 'very',\n",
       " 'simple',\n",
       " 'speedup',\n",
       " 'technique',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'based',\n",
       " 'language',\n",
       " 'models',\n",
       " 'negative',\n",
       " 'sampling',\n",
       " 'alternative',\n",
       " 'hierarchical',\n",
       " 'softmax',\n",
       " 'noise',\n",
       " 'contrastive',\n",
       " 'estimation',\n",
       " 'nce',\n",
       " 'which',\n",
       " 'was',\n",
       " 'troduced',\n",
       " 'gutmann',\n",
       " 'hyvarinen',\n",
       " 'applied',\n",
       " 'language',\n",
       " 'modeling',\n",
       " 'mnih',\n",
       " 'teh',\n",
       " 'nce',\n",
       " 'posits',\n",
       " 'good',\n",
       " 'model',\n",
       " 'should',\n",
       " 'differentiate',\n",
       " 'data',\n",
       " 'noise',\n",
       " 'means',\n",
       " 'logistic',\n",
       " 'regression',\n",
       " 'similar',\n",
       " 'hinge',\n",
       " 'loss',\n",
       " 'used',\n",
       " 'collobert',\n",
       " 'weston',\n",
       " 'who',\n",
       " 'trained',\n",
       " 'models',\n",
       " 'ranking',\n",
       " 'data',\n",
       " 'above',\n",
       " 'noise',\n",
       " 'nce',\n",
       " 'can',\n",
       " 'shown',\n",
       " 'approximately',\n",
       " 'maximize',\n",
       " 'log',\n",
       " 'probability',\n",
       " 'softmax',\n",
       " 'skip',\n",
       " 'gram',\n",
       " 'model',\n",
       " 'only',\n",
       " 'concerned',\n",
       " 'learning',\n",
       " 'high',\n",
       " 'quality',\n",
       " 'vector',\n",
       " 'representations',\n",
       " 'so',\n",
       " 'free',\n",
       " 'simplify',\n",
       " 'nce',\n",
       " 'long',\n",
       " 'vector',\n",
       " 'representations',\n",
       " 'retain',\n",
       " 'quality',\n",
       " 'dene',\n",
       " 'negative',\n",
       " 'sampling',\n",
       " 'neg',\n",
       " 'objective',\n",
       " 'log',\n",
       " 'wo',\n",
       " 'vwi',\n",
       " 'k',\n",
       " 'wipnw',\n",
       " 'log',\n",
       " 'wi',\n",
       " 'h',\n",
       " 'vwi',\n",
       " 'country',\n",
       " 'capital',\n",
       " 'vectors',\n",
       " 'projected',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = {\"\",\"s\",\"d\",\"y\", \"v\", \"j\", \"e\",\"g\", \"u\", \"t\", \"n\", \"c\", \"o\", \"l\", \"i\", \"a\", \"x\", \"b\", \"r\", \"cs\", \"ba\", \"aapo\", \"able\", \"zi\", \"ym\", \"of\", \"edu\", \"th\", \"it\", \"at\", \"or\", \"its\", \"you\", \"did\", \"pro\", \"such\", \"about\", \"thus\", \"or\", \"gmail\", \"with\",  \"arxiv\", \"our\", \"yee\", \"other\", \"from\", \"ing\", \"uses\", \"while\", \"but\", \"and\", \"ieee\", \"due\",\"els\", \"else\", \"their\", \"inc\", \"com\", \"the\", \"is\", \"an\", \"for\", \"that\", \"in\", \"we\", \"this\", \"by\", \"also\", \"to\", \"be\", \",\", \"et\", \"al\", \"figure\", \"on\", \"are\", \"as\", \"than\",\"kai\", \"aaai\", \"tion\"}\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    words = re.split(r\"[\\s*\\s\\.\\]\\[\\:\\@\\-]\" , text.lower())\n",
    "    for i in range(len(words)):\n",
    "        words[i] = re.sub(r\"[^a-z]\",'', words[i])\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "clean_text1 = text_preprocessing(text1)\n",
    "clean_text2 = text_preprocessing(text2)\n",
    "clean_text1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac68c31",
   "metadata": {},
   "source": [
    "#### Create bag of words model\n",
    "### Steps:\n",
    "- combine corpus in one array\n",
    "- make vocabulary\n",
    "- delete word that appearce only 1 times in both text to shrink bow mode(it will delete names and arrors that we do not need in our vector)\n",
    "- count word from vocabulary appearance in texts from corpus\n",
    "- return vector of counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c809f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 6, ..., 2, 4, 2]], dtype=uint32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bag_of_word(corpus):\n",
    "    full_texts =  [text for each_text in corpus for text in each_text]\n",
    "    min_appereance = 2\n",
    "    texts_dict = sorted(set(full_texts))\n",
    "    clean_dict = [word for word in texts_dict if full_texts.count(word) >= min_appereance]\n",
    "    vector = []\n",
    "    vector = np.zeros((len(corpus), len(clean_dict)), dtype=np.uint32)\n",
    "    for i, text in enumerate(corpus):\n",
    "        for j, word in enumerate(clean_dict):\n",
    "            vector[i][j] = text.count(word)\n",
    "    return vector, clean_dict\n",
    "\n",
    "bow, texts_dict = bag_of_word([clean_text1, clean_text2])\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6140d7",
   "metadata": {},
   "source": [
    "##### Calculate cosine similarity between articles\n",
    "- interpret measured cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "278cf1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.28 % similarity\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a,b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "cosine_similarity = cosine_sim(bow[0], bow[1])\n",
    "print(f\"{cosine_similarity * 100:.2f} % similarity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
